gemma3_narrative
A robust, multi-service application for narrative analysis powered by large language models (LLMs). This project uses a modular architecture with Docker Compose to deploy a FastAPI backend, a PostgreSQL database with vector capabilities, and a local Ollama LLM inference server. It also includes an Open WebUI for easy model interaction.

Features
FastAPI Backend: A high-performance Python API that provides endpoints for analyzing narrative data and interacting with the PostgreSQL database.

PostgreSQL with pgvector: A containerized database with pgvector for storing and querying vector embeddings generated by the LLMs.

Ollama GPU Inference: A custom Docker container that installs Ollama and downloads LLM models (gemma:3b, deepseek-llm:7b) to utilize your NVIDIA GPU for local inference.

Open WebUI: A web interface for directly interacting with the LLMs running on your Ollama server.

Agent-Based Architecture: Includes a scraper_agent and inserter_agent to automate data ingestion, analysis, and database population.

Prerequisites
To run this project, you need the following installed on your Linux machine:

Docker

Docker Compose

NVIDIA Container Toolkit (for GPU support)

Installation and Setup
Clone the repository:

git clone [https://github.com/cycotek/ollamaubnt24.git](https://github.com/cycotek/ollamaubnt24.git)
cd ollamaubnt24

Set up the environment file:
Open the .env file and replace the placeholder values with your own secure credentials. You can generate a secure API key using openssl rand -base64 32.

vim .env

Build and deploy the stack:
This command builds all custom Docker images and starts the entire multi-service application. It is designed to be idempotent and will not fail on subsequent runs.

docker compose up -d --build

Usage
Open WebUI
Open your web browser and navigate to http://localhost:8080. You can create a user, and the models you configured will be available for immediate use.

FastAPI API
Your FastAPI application is accessible at http://localhost:8000. The interactive API documentation is available at http://localhost:8000/docs.

To test the analyze_node endpoint, you can use curl with your API key.

curl -X 'GET' \
  'http://localhost:8000/analyze_node/1' \
  -H 'X-Api-Key: YOUR_SECRET_API_KEY'

(Replace YOUR_SECRET_API_KEY with the value from your .env file.)

Project File Structure
docker-compose.yml: The main deployment file for the entire stack.

.env: Stores environment variables for all services.

main.py: The core FastAPI application code.

database.py: Handles the database connection and configuration.

models.py: Defines the SQLAlchemy models for the database tables.

ollama/: Contains the custom Dockerfile and startup script for the Ollama service.

agents/: Contains the code and Dockerfiles for the scraper and inserter agents.

This README is ready for your repository. What's next?

version: '3.8'

services:
  # --- 1. Ollama GPU Service (LLM Inference) ---
  ollama:
    build:
      context: ./ollama
    container_name: gemma3_ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: always
    healthcheck:
      # Checks if the API is up, indicating models are being served (or downloading)
      test: ["CMD", "curl", "--fail", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 5

  # --- 2. Postgres with pgvector (Database Backend) ---
  db:
    image: ankane/pgvector:latest
    container_name: gemma3_db
    restart: always
    environment:
      # Creates the user and database from the .env file upon first launch
      POSTGRES_USER: ${DATABASE_USER}
      POSTGRES_PASSWORD: ${DATABASE_PASS}
      POSTGRES_DB: ${DATABASE_NAME}
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      # Checks if the DB is ready to accept TCP connections using the custom user
      test: ["CMD-SHELL", "pg_isready -U ${DATABASE_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- 3. FastAPI Main API Server (Application Logic) ---
  api_server:
    build: .
    container_name: gemma3_api
    restart: unless-stopped
    env_file:
      - .env
    # Wait for DB to be healthy and Ollama to be available before starting
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8000:8000"
    # Execute wait-for-postgres.sh to ensure DB is accepting connections before starting FastAPI
    command: /bin/sh -c "./wait-for-postgres.sh db 5432 && uvicorn main:app --host 0.0.0.0 --port 8000"
    healthcheck:
      # Checks if the FastAPI server is responsive
      test: ["CMD", "curl", "--fail", "http://localhost:8000/docs"]
      interval: 30s
      timeout: 10s
      retries: 5
    # NOTE: extra_hosts for host.docker.internal removed; containers communicate via service name 'db'

  # --- 4. Open WebUI (Model Interface) ---
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: gemma3_open-webui
    ports:
      - "8080:8080"
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      ollama:
        condition: service_healthy
    restart: always
    environment:
      # Connects to the Ollama service via Docker DNS
      - OLLAMA_HOST=http://ollama:11434

  # --- 5. Scraper Agent ---
  scraper_agent:
    build:
      context: .
      dockerfile: agents/scraper/Dockerfile
    container_name: gemma3_scraper
    restart: on-failure
    env_file:
      - .env
    # Wait for the API and Ollama before scraping
    depends_on:
      ollama:
        condition: service_healthy
      api_server:
        condition: service_healthy
    # Uses the internal Docker DNS name 'ollama' and internal host 'db' (from .env)
    command: ["python", "scraper.py"]

  # --- 6. Inserter Agent ---
  inserter_agent:
    build:
      context: .
      dockerfile: agents/insert_nodes/Dockerfile
    container_name: gemma3_inserter
    restart: on-failure
    env_file:
      - .env
    depends_on:
      api_server:
        condition: service_healthy
      ollama:
        condition: service_healthy
    # Uses the internal Docker DNS name 'db' (from .env)
    command: ["python", "agents/insert_nodes/insert_nodes.py"]

volumes:
  ollama_models:
  db_data:
  open-webui-data: